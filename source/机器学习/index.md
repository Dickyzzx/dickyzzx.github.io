---
title: æœºå™¨å­¦ä¹ 
date: 2024-08-03 00:12:10
---

# å‰è¨€
å¼€è¿™ä¸ªpageçš„æ˜¯å› ä¸ºå³å°†è¦å»è¯»ç ”äº†ï¼Œå½“ä½œä¸€æ¬¡å¤ä¹ æŠŠä¹‹å‰å­¦è¿‡çš„é‡æ–°å·©å›ºä¸€éï¼Œæ‰€ä»¥å¯¹äºç®€å•æ¦‚å¿µä¾æ—§ä¸ä¼šåšå¤ªå¤šè§£é‡Šï¼Œä½†æ˜¯å¦‚æœä½ ä¹Ÿæƒ³å­¦ä¹ æœºå™¨å­¦ä¹ å¯ä»¥æ ¹æ®æˆ‘çš„å†…å®¹è¿›è¡ŒæŸ¥æ¼è¡¥ç¼ºã€‚

## æœºå™¨å­¦ä¹ ç›¸å…³æœ¯è¯­
è¿™é‡Œä¼šéšæœºæ·»åŠ æƒ³èµ·æ¥æˆ–é‡åˆ°çš„æœ¯è¯­ï¼Œä¸éœ€è¦ä»”ç»†æŸ¥çœ‹ï¼Œä½†æ˜¯ç”¨ä½œæ¦‚å¿µå¤ä¹ æ˜¯å¾ˆå¥½çš„åŠæ³•ã€‚

    Model: An approximation of relationship between an input and output.

    Laplace Smoothing: A type of additive smoohting which mitigates the chance of encountering zero probabilities within the Naive Bayes classifier.

    Featurization: The process of transforming raw inputs into something a model can perform training and predictions on. åŒ…æ‹¬ä½†ä¸é™äºåç»­æåˆ°çš„0,1ï¼Œ2ï¼Œ3ï¼Œ

    Tokennization(0): The splitting of some raw textual input into individual words or elements.

    Stop word(1): A word, typically discarded, which doesn't add much predictive value, like this, is , a

    Stemming(2):Removing the ending modifiers of wards, leaving the stem of the word. studying ->study, studies -> studi

    Lemmatization(3): A more calculated form of stemming which ensures the proper lemma results from removing the word modifiers. studying->study, studies ->study ,but more expensive.

likelihood:ä¼¼ç„¶ç‡ï¼Œå…¶å®ç®—æ˜¯ä¸å¥½ç†è§£çš„ï¼Œä½†æ˜¯[è¿™ç¯‡æ–‡ç« ](https://blog.csdn.net/jh1137921986/article/details/89000994)è®²çš„å¥½

## Supervise learningï¼šç›‘ç£å­¦ä¹ 
### æœ´ç´ è´å¶æ–¯

    æ ¸å¿ƒç†å¿µ:æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯åŸºäºè´å¶æ–¯å®šç†å’Œç‰¹å¾ç‹¬ç«‹å‡è®¾çš„ç®€å•è€Œå¼ºå¤§çš„æ¦‚ç‡åˆ†ç±»å™¨ã€‚è´å¶æ–¯å®šç†ç»™å‡ºäº†åéªŒæ¦‚ç‡ ğ‘ƒ(ğ‘¦âˆ£ğ‘‹) çš„è®¡ç®—å…¬å¼ï¼Œå…¶ä¸­ X æ˜¯ç‰¹å¾çš„åˆï¼Œy æ˜¯ç±»åˆ«ã€‚ä¸æ‡‚çš„è¯·è‡ªè¡ŒGoogleï¼Œæœ‰å¤ªå¤šèµ„æºè®²çš„æ¯”æˆ‘å¥½äº†ã€‚
    è‡ªæˆ‘æ„Ÿè§‰éš¾ç‚¹åœ¨äº
    è”åˆæ¦‚ç‡P(a,b,c,d|y) = P(a|y)*...*P(d|y)
    åéªŒæ¦‚ç‡P(y|a,b,c,d) = 
    ï¼ï¼è¦è®¡ç®—åéªŒè¯æ¦‚ç‡éœ€è¦å…ˆè®¡ç®—è”åˆæ¦‚ç‡ã€‚

    å‡è®¾æˆ‘ä»¬çš„yåªæœ‰yeså’Œnoï¼Œa,b,c,dä¹Ÿæ˜¯0 or 1
    æˆ‘ä»¬è¦æ±‚P(y=yes|1 0 1 0) å’Œ P(y=no|1 0 1 0)
    ä¸€èˆ¬æ¥è¯´è¿‡ç¨‹å¦‚ä¸‹

    1.å…ˆéªŒæ¦‚ç‡ï¼šæ±‚P(y=yes) å’ŒP(y=no)

    2.æ¡ä»¶æ¦‚ç‡ï¼šåˆ†åˆ«æ±‚å„ä¸ªç»´åº¦given yes å’Œgiven noçš„æ¦‚ç‡, å¾—åˆ°P(a|y)...P(d|y) å’ŒP(a|n)...P(d|n)ã€‚

    3.ä¼¼ç„¶å‡½æ•°ï¼ˆè”åˆæ¦‚ç‡ï¼‰ï¼šæŠŠæ¡ä»¶æ¦‚ç‡æŒ‰yes å’Œno åˆ†ç±»ç›¸ä¹˜ã€‚å¾—åˆ°P(X|y=yes) å’ŒP(X|y=no)

    4.è¯æ®ï¼ˆè¾¹é™…ä¼¼ç„¶ï¼‰ï¼š P(ä¼¼ç„¶å‡½æ•°çš„yes) * P(å…ˆéªŒæ¦‚ç‡çš„yes) + P(ä¼¼ç„¶å‡½æ•°çš„no) * P(å…ˆéªŒæ¦‚ç‡çš„no)ï¼Œå¾—åˆ°P(a,b,c,d)

    5.åéªŒæ¦‚ç‡: åˆ°è¿™ä¸€æ­¥æˆ‘ä»¬å·²ç»æœ‰äº†æˆ‘ä»¬æ‰€éœ€è¦çš„ä¸€åˆ‡ ç›´æ¥å¥—å…¬å¼P(A|B)=....è®¡ç®—æˆ‘ä»¬è¦æ±‚çš„å³å¯ã€‚

    !!å› ä¸ºæœ´ç´ è´å¶æ–¯å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ æ‰€P(d|y)*P(c|d,y)*P(b|c,d,y)*P(a|,b,c,d,y) = p(a|y)*p(b|y)*p(c|y)*p(d|y)
    
    ä¸ºäº†é¿å…0æ¦‚ç‡é—®é¢˜ï¼ˆåˆ†å­ä¸º0ï¼‰å¯ä»¥ç”¨Laplace smoothingã€‚



### Performance
ç¡®å®šæ¨¡å‹è¡¨ç°çš„ä¸€äº›æ–¹æ³•ï¼šä»¥æ£€æµ‹ä¿¡æ¯æ˜¯å¦spamä¸ºä¾‹ã€‚

    å‡†ç¡®ç‡(Accuracy):(TP+TN)/Total ä½†æ˜¯å¹¶ä¸æ€»æ˜¯ä¸€ä¸ªå¥½çš„æŒ‡æ ‡ã€‚

    Sensitivity(recall): TP/(TP+FN) å¯ä»¥ç†è§£ä¸ºthe model'ablity to correctly classfiy spam message.

    Specificity: TN/(TN+FP) represnts the classifier's ability to correctly classify legitiamte message. 

    æ›´é«˜Specificityå°±ä¼šæ‹¥æœ‰æ›´å°‘çš„å‡é˜³ï¼Œæ›´é«˜çš„Sensitivityå°±ä¼šæ‹¥æœ‰æ›´å°‘çš„å‡é˜´ï¼Œæ ¹æ®ä¸åŒçš„æƒ…å†µä¼šæƒ³è¦ä¸ä¸€æ ·çš„å¹³è¡¡ã€‚å¹¶ä¸èƒ½ä¸€æ¦‚è€Œè®ºã€‚

    Precision: TP/(TP+FP),out of everytime, the model classified something as spam, how many of them actually were a spam. è¶Šé«˜è¶Šå¥½

    F1-Score: 2*(Sensitivity*Precision)/(Sensitivity+Precission), The hormonic mean of the sensitivety and the precision.

å…³äº è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†è¯·çœ‹[è¿™é‡Œ](https://blog.csdn.net/Swartz2015/article/details/78311592)



æœ´ç´ è´å¶æ–¯ä¼˜åŒ–

    Mutinomial Distribution : A distribution which models the probablity of counts of particular outcomes. å…·ä½“è¯·æœç´¢æœ´ç´ è´å¶æ–¯å’ŒMutinomial Distributionçš„ä¾‹å­ã€‚ é™¤äº†å¤šé¡¹å¼åˆ†å¸ƒï¼Œè¿˜æœ‰äºŒæ¬¡é¡¹åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒï¼Œç­‰ç­‰å»ºè®®éƒ½å»äº†è§£ä¸€éã€‚

    TF-IDF:è®¡ç®—TFå’ŒIDFçš„å€¼åç›¸ä¹˜ï¼Œå…·ä½“å¦‚ä½•è®¡ç®—è¯·è‡ªè¡ŒGoogleã€‚å…·ä½“æ¥è¯´ï¼Œtf-idfæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚ å­—è¯çš„é‡è¦æ€§éšç€å®ƒåœ¨æ–‡ä»¶ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”å¢åŠ ï¼Œä½†åŒæ—¶ä¼šéšç€å®ƒåœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”ä¸‹é™ã€‚æ¯”å¦‚æ¯ä¸ªæ–‡ç« éƒ½æœ‰travelè¿™ä¸ªè¯ï¼Œé‚£è¿™ä¸ªè¯çš„æ„ä¹‰å°±ä¸å¤§äº†ã€‚
    
    n-gram:è¯¦æƒ…è®¿é—®https://blog.csdn.net/weixin_44966641/article/details/127479910

    



k-ä¸´è¿‘ç®—æ³•

å„ç§è·ç¦»çš„è®¡ç®—å…¬å¼ï¼Œç‰¹å¾è§„èŒƒï¼Œç‰¹å¾æ ‡å‡†ç­‰
éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¸ª[kdæ ‘](https://blog.csdn.net/weixin_39910711/article/details/114447104)



å†³ç­–æ ‘
    æ€è€ƒå¦‚ä½•å¤„ç†:missing valueï¼Œmultiple lableï¼Œregressionã€‚
    Classification and Regreesion Tree(CART):
        is an algorithm for constructing an approximate optimal decision tree for given example.
    
    Split point:
        A paire of feture and fetures value which is assigned to a node in a decision tree.
        This split point will determine which examples will go left and which examples go right based on the feature and feature value.
    
    Gini Impurity:
        Used as a way to determine the best split point for a given node in classification tree. It's based on the probability of incorrectly classifying an item based on all of the items in the node. è¶Šä½è¶Šå¥½ã€‚ å¦‚æœæ˜¯ç”¨regressionåˆ™æ˜¯ç”¨mseæµ‹é‡ã€‚

    Surrogate Split:
        A suboptimal split point reserved for examples which are missing the optimal split point feature.
    
å…³äºBoostingå’ŒBaggingçœ‹[è¿™é‡Œ](https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E8%AF%A6%E8%A7%A3-bagging-boosting-%E4%BB%A5%E5%8F%8A%E4%BB%96%E4%BB%AC%E7%9A%84-4-%E7%82%B9%E5%8C%BA%E5%88%AB-6e3c72df05b8)


çº¿æ€§å›å½’

    Variance Inflation Factor
        A measure of multicollinearity in a regreesion model.

    Feature Interaction
        Features that are multiplied by on another in order to express relationships that can't be represnted by adding the independent variable terms together.


é€»è¾‘å›å½’

é€šä¿—ç†è§£ï¼Œæ¦‚ç‡åˆ†å¸ƒå‡½æ•°å’Œæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼š[é“¾æ¥ä¸€](https://cloud.tencent.com/developer/article/1514756)

åœ¨è¿ç»­å½¢å˜é‡ä¸­ï¼š

    æ¦‚ç‡åˆ†å¸ƒå‡½æ•°(CDF)æŒ‡çš„æ˜¯ç´¯ç§¯æ¦‚ç‡ï¼Œå¦‚æŸä¸ªäººçš„èº«é«˜å°äºæˆ–ç­‰äºæŸä¸ªå€¼çš„ç´¯ç§¯æ¦‚ç‡
    æ¦‚ç‡å¯†åº¦å‡½æ•°(PDF)æŒ‡çš„æ˜¯æŸä¸ªäººçš„èº«é«˜æ°å¥½åœ¨æŸä¸ªå€¼é™„è¿‘çš„â€œå¯èƒ½æ€§â€ã€‚
    
    Softmax
        A sigmoid which is generalized to more than 2 classes to be predicted against.


æ”¯æŒå‘é‡æœº