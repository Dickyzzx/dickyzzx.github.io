---
title: æœºå™¨å­¦ä¹ 
date: 2024-08-03 00:12:10
---

# å‰è¨€
å¼€è¿™ä¸ªpageçš„æ˜¯å› ä¸ºå³å°†è¦å»è¯»ç ”äº†ï¼Œå½“ä½œä¸€æ¬¡å¤ä¹ æŠŠä¹‹å‰å­¦è¿‡çš„é‡æ–°å·©å›ºä¸€éï¼Œæ‰€ä»¥å¯¹äºç®€å•æ¦‚å¿µä¾æ—§ä¸ä¼šåšå¤ªå¤šè§£é‡Šï¼Œä½†æ˜¯å¦‚æœä½ ä¹Ÿæƒ³å­¦ä¹ æœºå™¨å­¦ä¹ å¯ä»¥æ ¹æ®æˆ‘çš„å†…å®¹è¿›è¡ŒæŸ¥æ¼è¡¥ç¼ºã€‚

## æœºå™¨å­¦ä¹ ç›¸å…³æœ¯è¯­
è¿™é‡Œä¼šéšæœºæ·»åŠ æƒ³èµ·æ¥æˆ–é‡åˆ°çš„æœ¯è¯­ï¼Œä¸éœ€è¦ä»”ç»†æŸ¥çœ‹ï¼Œä½†æ˜¯ç”¨ä½œæ¦‚å¿µå¤ä¹ æ˜¯å¾ˆå¥½çš„åŠæ³•ã€‚

    Model: An approximation of relationship between an input and output.

    Laplace Smoothing: A type of additive smoohting which mitigates the chance of encountering zero probabilities within the Naive Bayes classifier.

    Featurization: The process of transforming raw inputs into something a model can perform training and predictions on. åŒ…æ‹¬ä½†ä¸é™äºåç»­æåˆ°çš„0,1ï¼Œ2ï¼Œ3ï¼Œ

    Tokennization(0): The splitting of some raw textual input into individual words or elements.

    Stop word(1): A word, typically discarded, which doesn't add much predictive value, like this, is , a

    Stemming(2):Removing the ending modifiers of wards, leaving the stem of the word. studying ->study, studies -> studi

    Lemmatization(3): A more calculated form of stemming which ensures the proper lemma results from removing the word modifiers. studying->study, studies ->study ,but more expensive.

likelihood:ä¼¼ç„¶ç‡ï¼Œå…¶å®ç®—æ˜¯ä¸å¥½ç†è§£çš„ï¼Œä½†æ˜¯[è¿™ç¯‡æ–‡ç« ](https://blog.csdn.net/jh1137921986/article/details/89000994)è®²çš„å¥½

## Supervise learningï¼šç›‘ç£å­¦ä¹ 
### æœ´ç´ è´å¶æ–¯

    æ ¸å¿ƒç†å¿µ:æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯åŸºäºè´å¶æ–¯å®šç†å’Œç‰¹å¾ç‹¬ç«‹å‡è®¾çš„ç®€å•è€Œå¼ºå¤§çš„æ¦‚ç‡åˆ†ç±»å™¨ã€‚è´å¶æ–¯å®šç†ç»™å‡ºäº†åéªŒæ¦‚ç‡ ğ‘ƒ(ğ‘¦âˆ£ğ‘‹) çš„è®¡ç®—å…¬å¼ï¼Œå…¶ä¸­ X æ˜¯ç‰¹å¾çš„åˆï¼Œy æ˜¯ç±»åˆ«ã€‚ä¸æ‡‚çš„è¯·è‡ªè¡ŒGoogleï¼Œæœ‰å¤ªå¤šèµ„æºè®²çš„æ¯”æˆ‘å¥½äº†ã€‚
    è‡ªæˆ‘æ„Ÿè§‰éš¾ç‚¹åœ¨äº
    è”åˆæ¦‚ç‡P(a,b,c,d|y) = P(a|y)*...*P(d|y)
    åéªŒæ¦‚ç‡P(y|a,b,c,d) = 
    ï¼ï¼è¦è®¡ç®—åéªŒè¯æ¦‚ç‡éœ€è¦å…ˆè®¡ç®—è”åˆæ¦‚ç‡ã€‚

    å‡è®¾æˆ‘ä»¬çš„yåªæœ‰yeså’Œnoï¼Œa,b,c,dä¹Ÿæ˜¯0 or 1
    æˆ‘ä»¬è¦æ±‚P(y=yes|1 0 1 0) å’Œ P(y=no|1 0 1 0)
    ä¸€èˆ¬æ¥è¯´è¿‡ç¨‹å¦‚ä¸‹

    1.å…ˆéªŒæ¦‚ç‡ï¼šæ±‚P(y=yes) å’ŒP(y=no)

    2.æ¡ä»¶æ¦‚ç‡ï¼šåˆ†åˆ«æ±‚å„ä¸ªç»´åº¦given yes å’Œgiven noçš„æ¦‚ç‡, å¾—åˆ°P(a|y)...P(d|y) å’ŒP(a|n)...P(d|n)ã€‚

    3.ä¼¼ç„¶å‡½æ•°ï¼ˆè”åˆæ¦‚ç‡ï¼‰ï¼šæŠŠæ¡ä»¶æ¦‚ç‡æŒ‰yes å’Œno åˆ†ç±»ç›¸ä¹˜ã€‚å¾—åˆ°P(X|y=yes) å’ŒP(X|y=no)

    4.è¯æ®ï¼ˆè¾¹é™…ä¼¼ç„¶ï¼‰ï¼š P(ä¼¼ç„¶å‡½æ•°çš„yes) * P(å…ˆéªŒæ¦‚ç‡çš„yes) + P(ä¼¼ç„¶å‡½æ•°çš„no) * P(å…ˆéªŒæ¦‚ç‡çš„no)ï¼Œå¾—åˆ°P(a,b,c,d)

    5.åéªŒæ¦‚ç‡: åˆ°è¿™ä¸€æ­¥æˆ‘ä»¬å·²ç»æœ‰äº†æˆ‘ä»¬æ‰€éœ€è¦çš„ä¸€åˆ‡ ç›´æ¥å¥—å…¬å¼P(A|B)=....è®¡ç®—æˆ‘ä»¬è¦æ±‚çš„å³å¯ã€‚

    !!å› ä¸ºæœ´ç´ è´å¶æ–¯å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ æ‰€P(d|y)*P(c|d,y)*P(b|c,d,y)*P(a|,b,c,d,y) = p(a|y)*p(b|y)*p(c|y)*p(d|y)
    
    ä¸ºäº†é¿å…0æ¦‚ç‡é—®é¢˜ï¼ˆåˆ†å­ä¸º0ï¼‰å¯ä»¥ç”¨Laplace smoothingã€‚



### Performance
ç¡®å®šæ¨¡å‹è¡¨ç°çš„ä¸€äº›æ–¹æ³•ï¼šä»¥æ£€æµ‹ä¿¡æ¯æ˜¯å¦spamä¸ºä¾‹ã€‚

    å‡†ç¡®ç‡(Accuracy):(TP+TN)/Total ä½†æ˜¯å¹¶ä¸æ€»æ˜¯ä¸€ä¸ªå¥½çš„æŒ‡æ ‡ã€‚

    Sensitivity(recall): TP/(TP+FN) å¯ä»¥ç†è§£ä¸ºthe model'ablity to correctly classfiy spam message.

    Specificity: TN/(TN+FP) represnts the classifier's ability to correctly classify legitiamte message. 

    æ›´é«˜Specificityå°±ä¼šæ‹¥æœ‰æ›´å°‘çš„å‡é˜³ï¼Œæ›´é«˜çš„Sensitivityå°±ä¼šæ‹¥æœ‰æ›´å°‘çš„å‡é˜´ï¼Œæ ¹æ®ä¸åŒçš„æƒ…å†µä¼šæƒ³è¦ä¸ä¸€æ ·çš„å¹³è¡¡ã€‚å¹¶ä¸èƒ½ä¸€æ¦‚è€Œè®ºã€‚

    Precision: TP/(TP+FP),out of everytime, the model classified something as spam, how many of them actually were a spam. è¶Šé«˜è¶Šå¥½

    F1-Score: 2*(Sensitivity*Precision)/(Sensitivity+Precission), The hormonic mean of the sensitivety and the precision.

å…³äº è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†è¯·çœ‹[è¿™é‡Œ](https://blog.csdn.net/Swartz2015/article/details/78311592)



æœ´ç´ è´å¶æ–¯ä¼˜åŒ–

k-ä¸´è¿‘ç®—æ³•

å†³ç­–æ ‘

çº¿æ€§å›å›½

é€»è¾‘è§„åˆ’

æ”¯æŒå‘é‡æœº